---
title: Insert title here
key: 5c90001e531292fbe0c05950a93f4e16

---
## Topic Modeling using Genism

```yaml
type: "TitleSlide"
key: "eef8d974b1"
```

`@lower_third`

name: Amit Manghani
title: Data Scientist


`@script`
Welcome to the course. In this video, you will learn about how to construct a Latent Dirichlet Allocation Model using Gensim. Latent Dirichlet allocation (LDA) is a topic model that generates topics based on word frequency from a set of documents. LDA is particularly useful for finding reasonably accurate mixtures of topics within a given document set.

Before we walk-through the process of generating a LDA model, let's quickly recap what we did in the last exercise from a data preparation perspective.


---
## LDA Walk-through - Import Data

```yaml
type: "FullSlide"
key: "e4a1ca7396"
center_content: false
```

`@part1`
Step #1: Import Sample Documents

```
doc_a = "Squash is good to eat. My brother likes to eat good squash, but not my father."

doc_b = "My father spends a lot of time driving my brother around to basketball practice."

doc_c = "Some health experts suggest that driving may cause increased tension and blood pressure."

doc_d = "I often feel pressure to perform well at school, but my father never pushes my brother to do better."

doc_e = "Health professionals say that squash is good for your health."

Compile sample documents into a list

doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]

```


`@script`
We read in 5 sample documents


---
## LDA Walk-through - Prepare Data

```yaml
type: "FullSlide"
key: "d1a497343b"
```

`@part1`
**1) Tokenization:**

```
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')

raw = doc_a.lower()
tokens = tokenizer.tokenize(raw)

print(tokens)
['squash', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'squash', 'but', 'not', 'my', 'father']
```


`@script`
In terms of preparing the data, we went through 3 steps:
1) Tokenization, 2) Removed stop words and 3) finally, performed stemming

Tokenization segments a document into its atomic elements. Tokenization can be performed in many ways – here, we are using NLTK’s tokenize.regexp module. The code snippet matches any word characters until it reaches a non-word character, like a space. 

An example output after document a has been tokenized is depicted. doc_a is now a list of tokens. In order to tokenize all the documents, we will need to create a for loop to traverse through all the documents - you will get an opportunity to do this in the exercise following this lecture module


---
## LDA Walk-through - Prepare Data

```yaml
type: "FullSlide"
key: "420218139d"
```

`@part1`
**2) Remove Stop Words:**

```
from stop_words import get_stop_words

# create English stop words list
en_stop = get_stop_words('en')

# remove stop words from tokens
stoppedwords_tokens = [i for i in tokens if not i in en_stop]

print(stoppedwords_tokens)

['squash', 'good', 'eat', 'brother', 'likes', 'eat', 'good', 'squash', 'father']
```


`@script`
Certain parts of English speech, like conjunctions ("the", “for”, “or”) are meaningless or inconsequential to a topic model. These terms are called stop words and need to be removed from our token list.

In this case, we are using the stop_words package - we call the get_stop_words() to create a list of stop words. 

Removing stop words is just a matter of looping through the tokenized list of words and comparing each word to the en_stop list.

Stop words are eliminated from the list of tokens which "document a"was converted into in the previous step


---
## LDA Walk-through - Prepare Data

```yaml
type: "FullSlide"
key: "b149def40d"
```

`@part1`
**3) Perform Stemming:**

```
from nltk.stem.porter import PorterStemmer

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

# stem token
stemmed_tokens = [p_stemmer.stem(i) for i in stoppedwords_tokens]

print(stemmed_tokens)

['squash', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'squash', 'father']
```


`@script`
Stemming words is a common Natural Language Processing technique to reduce topically similar words to their root. For example, “stemming,” “stemmer,” “stemmed,” all have similar meanings; stemming reduces those terms to “stem.” 

This is important for topic modeling; otherwise, the terms "stemming", "stemmer" and "stemmed" would be viewed as separate entities and consequently their importance in the model would be reduced.

In this case, the Porter stemming algorithm is used method. To implement a Porter stemming algorithm, import the Porter Stemmer module from NLTK.

The Porter Stemmer requires all tokens to be type string. We loop through stopped_tokens to stem each of the respective tokens in the list. In this case, you will notice that the word "likes" became "like".


---
## LDA Walk-through - Create Document-Term Matrix

```yaml
type: "FullSlide"
key: "b76df10dda"
```

`@part1`
```
# add tokens to list
texts.append(stemmed_tokens)

# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts)

print(dictionary.token2id)

{'brother': 0, 'eat': 1, 'father': 2, 'good': 3, 'like': 4, 'squash': 5, 'around': 6, 'basketbal': 7, 'drive': 8, 'lot': 9, 'practic': 10, 'spend': 11, 'time': 12, 'blood': 13, 'caus': 14, 'expert': 15, 'health': 16, 'increas': 17, 'may': 18, 'pressur': 19, 'suggest': 20, 'tension': 21, 'better': 22, 'feel': 23, 'never': 24, 'often': 25, 'perform': 26, 'push': 27, 'school': 28, 'well': 29, 'profession': 30, 'say': 31}
```
**Convert dictionary into a bag-of-words:**

```
corpus = [dictionary.doc2bow(text) for text in texts]

print(corpus[0])

[(0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 2)]
```


`@script`
The result of the data preparation stage is stored in stemmed_tokens which is a tokenized, stopped and stemmed list of words from a single document. 

Let’s say  that we have looped through all the 5 documents that we imported in step 1 and appended each of the stemmed_token to texts. Now, texts is a list of lists, one list for each of our original documents.

Next, we need to understand how frequently each term occurs within each document. To do that, we need to construct a document-term matrix with a package called genism.

The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also gathering word counts.
print(dictionary.token2id) enables us to see each token’s unique integer id


---
## LDA Walk-through - Generate a LDA Model

```yaml
type: "FullSlide"
key: "75624c1d9e"
```

`@part1`
**Construct LDA model using Genism:**

```
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)
```
**Parameters:**

•	num_topics is required

•	id2word is required

•	passes is optional


`@script`



---
## LDA Walk-through - Review Topics

```yaml
type: "FullSlide"
key: "1bc8990499"
```

`@part1`
**Examine Topics:**

```

print(ldamodel.print_topics(num_topics=3, num_words=3))

[(0, '0.076*"brother" + 0.076*"father" + 0.043*"pressur"'), (1, '0.079*"squash" + 0.079*"good" + 0.079*"eat"'), (2, '0.141*"health" + 0.080*"profession" + 0.080*"say"')]

```

**Inferring Topics:**

Topic 1 --> Family

Topic 2 --> Vegetables

Topic 3 --> Health


`@script`



---
## Let's Practice LDA

```yaml
type: "FinalSlide"
key: "2ca7f262c8"
```

`@script`


